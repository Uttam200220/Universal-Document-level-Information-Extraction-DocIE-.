{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51afff81",
   "metadata": {},
   "source": [
    "XLLM-ACL Named Entity Recognition Challenge Project\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72576ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========= SET THESE PATHS =========\n",
    "\n",
    "PROJECT_ROOT = Path(\".\")  # current folder\n",
    "\n",
    "TRAIN_DIR = PROJECT_ROOT / \"train\"\n",
    "DEV_DIR   = PROJECT_ROOT / \"dev\"\n",
    "TEST_DIR  = PROJECT_ROOT / \"test\"  \n",
    "\n",
    "# ========= MODEL / TRAINING CONFIG =========\n",
    "BASE_NER_MODEL = \"allenai/longformer-base-4096\"  \n",
    "MAX_LEN = 2048   \n",
    "BATCH_SIZE = 1   \n",
    "NER_EPOCHS = 5   \n",
    "LR = 2e-5        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fb1967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 51 train docs\n",
      "Loaded 23 dev docs\n",
      "Example train doc keys: dict_keys(['domain', 'title', 'doc', 'entities', 'triples', 'label_set', 'entity_label_set'])\n"
     ]
    }
   ],
   "source": [
    "# Recursively load all *.json files under dir_path. Each file can be a list[dict] or a single dict. \n",
    "def load_docs_from_dir(dir_path: Path):    \n",
    "    docs = []\n",
    "    for jp in sorted(dir_path.rglob(\"*.json\")):\n",
    "        with open(jp, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            docs.extend(data)\n",
    "        elif isinstance(data, dict):\n",
    "            docs.append(data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected JSON structure in {jp}\")\n",
    "    return docs\n",
    "\n",
    "train_docs = load_docs_from_dir(TRAIN_DIR)\n",
    "dev_docs   = load_docs_from_dir(DEV_DIR)\n",
    "\n",
    "print(f\"Loaded {len(train_docs)} train docs\")\n",
    "print(f\"Loaded {len(dev_docs)} dev docs\")\n",
    "print(\"Example train doc keys:\", train_docs[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcee04ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity types: ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MISC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
      "Num NER labels: 39\n",
      "Sample labels: ['O', 'B-CARDINAL', 'I-CARDINAL', 'B-DATE', 'I-DATE', 'B-EVENT', 'I-EVENT', 'B-FAC', 'I-FAC', 'B-GPE', 'I-GPE', 'B-LANGUAGE', 'I-LANGUAGE', 'B-LAW', 'I-LAW', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-MONEY']\n"
     ]
    }
   ],
   "source": [
    "# Collect entity types from TRAIN + DEV data\n",
    "entity_types = set()\n",
    "\n",
    "for d in train_docs + dev_docs:\n",
    "    if \"entity_label_set\" in d:\n",
    "        entity_types.update(d[\"entity_label_set\"])\n",
    "    for ent in d.get(\"entities\", []):\n",
    "        t = ent.get(\"type\")\n",
    "        if t:\n",
    "            entity_types.add(t)\n",
    "\n",
    "entity_types = sorted(entity_types)\n",
    "print(\"Entity types:\", entity_types)\n",
    "\n",
    "ner_labels = [\"O\"]\n",
    "for t in entity_types:\n",
    "    ner_labels.append(f\"B-{t}\")\n",
    "    ner_labels.append(f\"I-{t}\")\n",
    "\n",
    "label2id_ner = {lbl: i for i, lbl in enumerate(ner_labels)}\n",
    "id2label_ner = {i: lbl for lbl, i in label2id_ner.items()}\n",
    "\n",
    "print(\"Num NER labels:\", len(ner_labels))\n",
    "print(\"Sample labels:\", ner_labels[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18958400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon size: 3445\n",
      "Lexicon sample: [('william thomson, 1st baron kelvin', 'PERSON'), ('william thomson', 'PERSON'), ('1st baron kelvin', 'PERSON'), ('kelvin', 'PERSON'), ('baron kelvin', 'PERSON'), ('1st', 'ORDINAL'), ('cable signals', 'MISC'), ('wheatstone transmitter', 'MISC'), ('land line', 'MISC'), ('long cable', 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "mention_type_counts = defaultdict(Counter)\n",
    "\n",
    "for doc in train_docs:\n",
    "    for ent in doc.get(\"entities\", []):\n",
    "        t = ent.get(\"type\")\n",
    "        for m in ent.get(\"mentions\", []):\n",
    "            if not m:\n",
    "                continue\n",
    "            mention_type_counts[m.lower()][t] += 1\n",
    "\n",
    "lexicon = {\n",
    "    m: cnt.most_common(1)[0][0]\n",
    "    for m, cnt in mention_type_counts.items()\n",
    "}\n",
    "\n",
    "print(\"Lexicon size:\", len(lexicon))\n",
    "sample_lex = list(lexicon.items())[:10]\n",
    "print(\"Lexicon sample:\", sample_lex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f5b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example spans: [(5, 26, 'MISC'), (41, 54, 'MISC'), (68, 83, 'PERSON'), (68, 101, 'PERSON'), (85, 88, 'ORDINAL'), (85, 101, 'PERSON'), (89, 101, 'PERSON'), (95, 101, 'PERSON'), (153, 158, 'MISC'), (178, 200, 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "# From entity mentions, build a list of (start_char, end_char, type). Uses regex with word-boundary heuristics to avoid mid-word matches.\n",
    "def build_char_spans(doc_text: str, entities: list):    \n",
    "    spans = []\n",
    "\n",
    "    for ent in entities:\n",
    "        ent_type = ent.get(\"type\")\n",
    "        if not ent_type:\n",
    "            continue\n",
    "\n",
    "        for m in ent.get(\"mentions\", []):\n",
    "            if not m:\n",
    "                continue\n",
    "\n",
    "            pattern = re.escape(m)\n",
    "            \n",
    "            if m[0].isalnum() and m[-1].isalnum():\n",
    "                pattern = r\"\\b\" + pattern + r\"\\b\"\n",
    "\n",
    "            for match in re.finditer(pattern, doc_text):\n",
    "                start, end = match.span()\n",
    "                spans.append((start, end, ent_type))\n",
    "\n",
    "    spans.sort(key=lambda x: (x[0], x[1]))\n",
    "    return spans\n",
    "\n",
    "# quick sanity on first train doc\n",
    "sample_spans = build_char_spans(train_docs[0][\"doc\"], train_docs[0].get(\"entities\", []))\n",
    "print(\"Example spans:\", sample_spans[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891c51a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 51\n",
      "Dev examples: 23\n"
     ]
    }
   ],
   "source": [
    "ner_tokenizer = AutoTokenizer.from_pretrained(BASE_NER_MODEL)\n",
    "\n",
    "# Convert docs to tokenized inputs + BIO label IDs. One Longformer pass per document (up to max_length tokens).\n",
    "\n",
    "def encode_ner_docs(docs, tokenizer, max_length, label2id):    \n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_label_ids = []\n",
    "\n",
    "    for d in docs:\n",
    "        text = d[\"doc\"]\n",
    "        if not isinstance(text, str):\n",
    "            if isinstance(text, (list, tuple)):\n",
    "                text = \" \".join(str(x) for x in text)\n",
    "            else:\n",
    "                text = str(text)\n",
    "\n",
    "        entities = d.get(\"entities\", [])\n",
    "        spans = build_char_spans(text, entities)\n",
    "\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "        offsets = enc[\"offset_mapping\"]\n",
    "\n",
    "        labels = [\"O\"] * len(input_ids)\n",
    "\n",
    "        \n",
    "        for (span_start, span_end, span_type) in spans:\n",
    "            inside = False\n",
    "            for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "                if tok_end <= tok_start:  \n",
    "                    continue\n",
    "\n",
    "                if tok_start >= span_end:\n",
    "                    break  \n",
    "\n",
    "                if tok_start >= span_start and tok_end <= span_end:\n",
    "                    if not inside:\n",
    "                        labels[i] = f\"B-{span_type}\"\n",
    "                        inside = True\n",
    "                    else:\n",
    "                        labels[i] = f\"I-{span_type}\"\n",
    "\n",
    "        # Convert to IDs; ignore special tokens with -100\n",
    "        label_ids = []\n",
    "        for lab, (tok_start, tok_end) in zip(labels, offsets):\n",
    "            if tok_end <= tok_start:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label2id.get(lab, label2id[\"O\"]))\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_label_ids.append(label_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks,\n",
    "        \"labels\": all_label_ids,\n",
    "    }\n",
    "\n",
    "train_encodings = encode_ner_docs(train_docs, ner_tokenizer, MAX_LEN, label2id_ner)\n",
    "dev_encodings   = encode_ner_docs(dev_docs,   ner_tokenizer, MAX_LEN, label2id_ner)\n",
    "\n",
    "print(\"Train examples:\", len(train_encodings[\"input_ids\"]))\n",
    "print(\"Dev examples:\", len(dev_encodings[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3e048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 51\n",
      "Dev dataset size: 23\n"
     ]
    }
   ],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return lists; DataCollator pads & tensorizes\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.encodings[\"labels\"][idx],\n",
    "        }\n",
    "\n",
    "train_dataset = NERDataset(train_encodings)\n",
    "dev_dataset   = NERDataset(dev_encodings)\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Dev dataset size:\", len(dev_dataset))\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=ner_tokenizer,\n",
    "    padding=True,\n",
    "    max_length=MAX_LEN,\n",
    "    return_tensors=\"pt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f77797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts (id -> count): Counter({0: 42757, 18: 2622, 17: 1703, 26: 1210, 30: 1179, 3: 676, 25: 642, 29: 629, 4: 377, 38: 353, 9: 333, 22: 326, 10: 316, 1: 260, 21: 226, 32: 158, 37: 150, 2: 136, 31: 131, 6: 118, 8: 94, 23: 85, 34: 82, 16: 70, 7: 69, 20: 55, 27: 48, 15: 47, 28: 43, 5: 42, 33: 41, 24: 41, 14: 41, 19: 22, 13: 13, 12: 11, 11: 10, 35: 7, 36: 7})\n",
      "Class weights: tensor([3.3061e-02, 5.4369e+00, 1.0394e+01, 2.0911e+00, 3.7496e+00, 3.3657e+01,\n",
      "        1.1980e+01, 2.0487e+01, 1.5038e+01, 4.2450e+00, 4.4734e+00, 1.4136e+02,\n",
      "        1.2851e+02, 1.0874e+02, 3.4478e+01, 3.0076e+01, 2.0194e+01, 8.3006e-01,\n",
      "        5.3913e-01, 6.4254e+01, 2.5702e+01, 6.2548e+00, 4.3362e+00, 1.6630e+01,\n",
      "        3.4478e+01, 2.2019e+00, 1.1683e+00, 2.9450e+01, 3.2874e+01, 2.2474e+00,\n",
      "        1.1990e+00, 1.0791e+01, 8.9468e+00, 3.4478e+01, 1.7239e+01, 2.0194e+02,\n",
      "        2.0194e+02, 9.4239e+00, 4.0045e+00], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten labels (ignore -100)\n",
    "all_labels_flat = [\n",
    "    lab\n",
    "    for seq in train_encodings[\"labels\"]\n",
    "    for lab in seq\n",
    "    if lab != -100\n",
    "]\n",
    "\n",
    "label_counts = Counter(all_labels_flat)\n",
    "print(\"Label counts (id -> count):\", label_counts)\n",
    "\n",
    "num_labels = len(ner_labels)\n",
    "class_weights = torch.ones(num_labels, dtype=torch.float)\n",
    "\n",
    "total = sum(label_counts.values())\n",
    "for lab_id, count in label_counts.items():\n",
    "    class_weights[lab_id] = total / (num_labels * count)\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "125358db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongformerForTokenClassification(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=39, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    BASE_NER_MODEL,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label_ner,\n",
    "    label2id=label2id_ner,\n",
    "    ignore_mismatched_sizes=True, \n",
    ")\n",
    "\n",
    "ner_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0056a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class WeightedNERTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # accept extra kwargs like num_items_in_batch\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  \n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(\n",
    "            weight=self.class_weights,\n",
    "            ignore_index=-100,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cfa2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/ner_longformer\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NER_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,           \n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229edffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eldra\\OneDrive\\Desktop\\ydutS\\626tiA\\Project\\nlp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 02:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.791600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.844200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.727500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eldra\\OneDrive\\Desktop\\ydutS\\626tiA\\Project\\nlp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eldra\\OneDrive\\Desktop\\ydutS\\626tiA\\Project\\nlp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev metrics: {'eval_loss': 2.366665840148926, 'eval_runtime': 50.0017, 'eval_samples_per_second': 0.46, 'eval_steps_per_second': 0.06, 'epoch': 5.0}\n",
      "Saved fine-tuned model to: outputs/ner_longformer_model\n"
     ]
    }
   ],
   "source": [
    "ner_trainer = WeightedNERTrainer(\n",
    "    model=ner_model,\n",
    "    args=ner_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,   \n",
    "    data_collator=data_collator,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "\n",
    "ner_trainer.train()\n",
    "\n",
    "metrics = ner_trainer.evaluate()\n",
    "print(\"Dev metrics:\", metrics)\n",
    "\n",
    "save_dir = \"outputs/ner_longformer_model\"\n",
    "ner_trainer.save_model(save_dir)\n",
    "ner_tokenizer.save_pretrained(save_dir)\n",
    "print(\"Saved fine-tuned model to:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic filters to remove obvious junk spans.\n",
    "def clean_predicted_entities(entities):    \n",
    "    cleaned = []\n",
    "    for ent in entities:\n",
    "        ent_type = ent[\"type\"]\n",
    "        new_mentions = []\n",
    "        for m in ent[\"mentions\"]:\n",
    "            m_strip = m.strip()\n",
    "            if len(m_strip) < 3:\n",
    "                continue\n",
    "            if all(ch in \",.;:-_()[]{}\\\"'\" for ch in m_strip):\n",
    "                continue\n",
    "            if \" \" not in m_strip and m_strip.islower() and len(m_strip) <= 4:\n",
    "                continue\n",
    "            new_mentions.append(m_strip)\n",
    "        if new_mentions:\n",
    "            cleaned.append({\n",
    "                \"type\": ent_type,\n",
    "                \"mentions\": sorted(set(new_mentions)),\n",
    "            })\n",
    "    return cleaned\n",
    "\n",
    "# Predict DocIE-style entities for one document, BIO decode spans, group by (mention, type), optional lexicon-based type correction\n",
    "def ner_predict_entities(\n",
    "    doc_text,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    id2label,\n",
    "    max_length=MAX_LEN,\n",
    "    lexicon=None,\n",
    "):\n",
    "    if not isinstance(doc_text, str):\n",
    "        if isinstance(doc_text, (list, tuple)):\n",
    "            doc_text = \" \".join(str(x) for x in doc_text)\n",
    "        else:\n",
    "            doc_text = str(doc_text)\n",
    "\n",
    "    model.eval()\n",
    "    all_spans = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc = tokenizer(\n",
    "            doc_text,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        offsets = enc.pop(\"offset_mapping\")[0].cpu().tolist()\n",
    "        outputs = model(**enc)\n",
    "        pred_ids = outputs.logits.argmax(-1)[0].cpu().tolist()\n",
    "\n",
    "    current_type = None\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "\n",
    "    for label_id, (start_char, end_char) in zip(pred_ids, offsets):\n",
    "        if end_char <= start_char:\n",
    "            continue  \n",
    "\n",
    "        tag = id2label[label_id]\n",
    "        if tag == \"O\":\n",
    "            if current_type is not None:\n",
    "                all_spans.append((current_start, current_end, current_type))\n",
    "                current_type = None\n",
    "                current_start = None\n",
    "                current_end = None\n",
    "            continue\n",
    "\n",
    "        prefix, ent_type = tag.split(\"-\", 1)\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            if current_type is not None:\n",
    "                all_spans.append((current_start, current_end, current_type))\n",
    "            current_type = ent_type\n",
    "            current_start = start_char\n",
    "            current_end = end_char\n",
    "        elif prefix == \"I\":\n",
    "            if current_type == ent_type:\n",
    "                current_end = end_char\n",
    "            else:\n",
    "                if current_type is not None:\n",
    "                    all_spans.append((current_start, current_end, current_type))\n",
    "                current_type = ent_type\n",
    "                current_start = start_char\n",
    "                current_end = end_char\n",
    "\n",
    "    if current_type is not None:\n",
    "        all_spans.append((current_start, current_end, current_type))\n",
    "\n",
    "    # Deduplicate spans\n",
    "    span_set = set(all_spans)\n",
    "\n",
    "    grouped = defaultdict(set)\n",
    "    for (s, e, t) in span_set:\n",
    "        mention = doc_text[s:e].strip()\n",
    "        if mention:\n",
    "            grouped[(mention, t)].add(mention)\n",
    "\n",
    "    entities = []\n",
    "    for (mention, ent_type), mentions in grouped.items():\n",
    "        entities.append({\n",
    "            \"mentions\": list(mentions),\n",
    "            \"type\": ent_type,\n",
    "        })\n",
    "\n",
    "    entities = clean_predicted_entities(entities)\n",
    "\n",
    "    # Lexicon-based type override\n",
    "    if lexicon is not None:\n",
    "        for ent in entities:\n",
    "            votes = Counter()\n",
    "            for m in ent[\"mentions\"]:\n",
    "                key = m.lower()\n",
    "                if key in lexicon:\n",
    "                    votes[lexicon[key]] += 1\n",
    "            if votes:\n",
    "                ent[\"type\"] = votes.most_common(1)[0][0]\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130a3c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23,\n",
       " ['Human_behavior_0',\n",
       "  'Human_behavior_1',\n",
       "  'Human_behavior_2',\n",
       "  'Human_behavior_3',\n",
       "  'Human_behavior_4'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create IDs for dev docs for later analysis\n",
    "dev_ids = [f\"{doc['domain']}_{i}\" for i, doc in enumerate(dev_docs)]\n",
    "len(dev_ids), dev_ids[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "add138b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dev reference to: input\\ref\\reference.json\n"
     ]
    }
   ],
   "source": [
    "# Build reference.json (GROUND TRUTH) for dev\n",
    "reference = {}\n",
    "\n",
    "for doc_id, doc in zip(dev_ids, dev_docs):\n",
    "    \n",
    "    reference[doc_id] = doc\n",
    "\n",
    "ref_out_dir = Path(\"input\") / \"ref\"\n",
    "ref_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "ref_file = ref_out_dir / \"reference.json\"\n",
    "\n",
    "with open(ref_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reference, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Wrote dev reference to:\", ref_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfb769cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dev predictions to: input\\res\\results.json\n"
     ]
    }
   ],
   "source": [
    "# Build results.json (PREDICTIONS) for dev \n",
    "results = {}\n",
    "\n",
    "for doc_id, doc in zip(dev_ids, dev_docs):\n",
    "    text = doc[\"doc\"]\n",
    "    title = doc.get(\"title\", \"\")\n",
    "    ents = ner_predict_entities(text, ner_tokenizer, ner_model, id2label_ner, max_length=MAX_LEN, lexicon=lexicon)\n",
    "\n",
    "    results[doc_id] = {\n",
    "        \"title\":   title,\n",
    "        \"entities\": ents,\n",
    "        \"triples\": [],   \n",
    "    }\n",
    "\n",
    "res_out_dir = Path(\"input\") / \"res\"\n",
    "res_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "res_file = res_out_dir / \"results.json\"\n",
    "\n",
    "with open(res_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Wrote dev predictions to:\", res_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
